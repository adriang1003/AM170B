{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8bc5faf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression and Model Selection\n",
    "By: Adrian Garcia <br>\n",
    "UCSC: AM-170B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8843d97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "1. Implement a linear regression algorithm using your implementation of gradient descent for the following objective/loss functions:\n",
    "<br>\n",
    "    a. Mean absolute error $\\ell_{1}:E_{1}(f) = \\sum_{k = 1}^{n}\\lvert f(x_{k}) - y_{k}\\lvert$,\n",
    "<br>\n",
    "    b. Least squared error $\\ell_{2}:E_{2}(f) = \\left(\\sum_{k = 1}^{n}\\lvert f(x_{k}) - y_{k}\\lvert\\right)^{\\frac{1}{2}}$,\n",
    "<br>\n",
    "where $f(x) = mx - b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f9976",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Road Map\n",
    "1. Create **two** linear regression functions that:\n",
    "<br>\n",
    "(a) Uses the specified loss function for gradient descent.\n",
    "<br>\n",
    "(b) Takes in inputs: ($x$, $y$, $m_0$, $b_0$, $\\delta$, epochs).\n",
    "<br>\n",
    "(b) Produces outputs: ($m$, $b$, error).\n",
    "2. Load in data into our main program, isolate desired data, and **split data into training/testing sets**.\n",
    "3. Train algorithm(s) with specified arguements and plot desired results.\n",
    "<br>\n",
    "<font color='red'>NOTE</font>: epochs -> # of iterations training data is passed through an algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8265c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "def algorithm(x, y, m, b, L, epochs = 1000):\n",
    "    # Initialize error array\n",
    "    for i in range(epochs):\n",
    "        # Define model: f(x) = mx - b\n",
    "        # Calculate the partial derivative of the loss function w.r.t m\n",
    "        # Calculate the partial derivative of the loss function w.r.t b\n",
    "        m = m - L * dldm # Update m\n",
    "        b = b - L * dldb # Update b\n",
    "        err[i] = sum(abs(y_pred - y)) or sum(abs(y_pred - y))**(1/2) # Calculate error\n",
    "    return m, b, err # Return calculated coefficients and error (array)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c26957b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caveats\n",
    "1. Too small of a learning rate -> takes too long to reach a minimum.\n",
    "2. Too large of a learning rate -> drastic updates/divergent behavior\n",
    "3. Gradient descent does **not** guarentee that the minimum you are reaching is the **global** minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c874da4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Test your algorithm on the dataset almost_linear.csv (fit one set of data either TV vs sales or Radio vs sales) using at most three different learning rates. Plot $\\ell_{1}$ and $\\ell_{2}$ errors as a function of number of iterations of your algorithm. Plot learning rate as a function of the number of iterations in your algorithm. State values for $m$, $b$, and total error for each objective/loss function and learning rate combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f0c3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimal learning rate\n",
    "For most applications, there exists an optimal learning rate for gradient descent at each iteration step; however, only a subset of those applications allow for a computationally inexpensive search for that rate. So, assuming we could (and want to):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17b041",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $f(x)$ be the function gradient descent is being applied to.\n",
    "<br>\n",
    "At each iteration:\n",
    "$$\n",
    "x_{k + 1}(\\delta) = x_{k} - \\delta\\nabla f(x_{k})\n",
    "$$\n",
    "In using this expression, we have:\n",
    "$$\n",
    "F(\\delta) = f(x_{k + 1}(\\delta))\n",
    "$$\n",
    "Thus, the optimal learning rate can be found by setting $\\frac{dF}{d\\delta} = 0$ and solving for $\\delta$. Hence,\n",
    "$$\n",
    "\\frac{dF}{d\\delta} = -\\nabla f(x_{k + 1})\\nabla f(x_{k}) = 0 \\implies \\delta = \\dots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed8243",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "def algorithm(x, y, m, b, L, epochs = 1000, opt_L = False):\n",
    "    if (opt_L == True):\n",
    "        # Initialize error and learning rate array\n",
    "        for i in range(epochs):\n",
    "            # Find the optimal learning rate\n",
    "            # Repeat previously illustrated code\n",
    "    else:\n",
    "        # Previously illustrated code\n",
    "    return m, b, err, L # Return ... and learning rate (array or float)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76176326",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. Compare your results with a standard linear regression solver like sklearn.linear_model.LinearRegression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c03099",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Selection\n",
    "1. Explore the data (https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). Plot the distribution of median house value. Plot median house value vs total bedrooms. Plot the correlation matrix. Discuss plot results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc4b51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "# Load in data\n",
    "data = pd.read_csv('cal_housing.data', header = None)\n",
    "# Set column names\n",
    "data.columns = [...]\n",
    "sns.histplot() # Plot data 1\n",
    "sns.scatterplot() # Plot data 2\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "corr_matrix = data.corr() # Find correlation matrix\n",
    "sns.heatmap() # Plot data 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b7179",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Explore three different linear regression models: these can include least squares, lasso, ridge, elastic net, or any other linear (or nonlinear) regression model. Perform a grid search on the regularization parameters, and use five fold cross validation to find parameter values that minimize the error on your test data for each of the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0be55a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### k-Fold Cross Validation\n",
    "**Problem**: How can we compare different models, or the same model with different parameter(s) ;), in a fair and robust way?\n",
    "<br>\n",
    "<br>\n",
    "**Idea**: Partition a dataset into a training set and a test set $k$ different ways and average the \"scores\" of each of the models (or parameters) and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86db664",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid Search\n",
    "**Problem**: What is the best parameter, or parameters, for a specific model?\n",
    "<br>\n",
    "<br>\n",
    "**Idea**: Build a model with **every** possible parameter value (defined by the user), and compare which parameter value performed the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bebfce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "# Import packages\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameter we wish to optimize\n",
    "parameters = {'alpha':np.linspace(start, stop, number)}\n",
    "Lasso = linear_model.Lasso() # Define the model we wish to test\n",
    "# Define a grid search that will use 5-fold cross validation\n",
    "Lasso_reg = GridSearchCV(Lasso, parameters, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "Lasso_reg.fit(X,Y) # Fit the grid search\n",
    "print(Lasso_reg.best_estimator_) # Print best alpha parameter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42f4e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. (cont.) For each model, plot a) regularization parameter value vs. error on test data and error on training data b) regularization parameter value vs predictor coefficient values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde5512",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "alphas = np.linspace(start, stop, number)\n",
    "mse = []\n",
    "coef = []\n",
    "for a in alphas:\n",
    "    # Create linear regression object\n",
    "    # Train the model using the training set\n",
    "    # Make prediction using the training (or testing) set\n",
    "    mean = mean_squared_error(Y_test, Y_pred) # Calculate the error\n",
    "    mse.append(mean) # add error to list\n",
    "    coef.append(model.coef_) # add predictor coefficent to list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721f4eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code (a)\n",
    "```python\n",
    "plt.figure()\n",
    "plt.plot(alphas, mse, label = 'Model')\n",
    "# Plot config\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955c14a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code (b)\n",
    "```python\n",
    "plt.figure()\n",
    "plt.plot(alphas, coef, label = 'Model')\n",
    "# Plot config\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830605c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. Plot test errors for all three models for the optimal parameters chosen in question 2. Choose the model that best fits the data. Discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eccd1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "plt.bar([Models], [Errors for the optimal paramters])\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
