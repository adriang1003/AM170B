{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8bc5faf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression and Model Selection\n",
    "By: Adrian Garcia <br>\n",
    "UCSC: AM-170B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8843d97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "1. Implement a linear regression algorithm using your implementation of gradient descent for the following objective/loss functions:\n",
    "<br>\n",
    "    a. Mean absolute error $\\ell_{1}:E_{1}(f) = \\sum_{k = 1}^{n}\\lvert f(x_{k}) - y_{k}\\lvert$,\n",
    "<br>\n",
    "    b. Least squared error $\\ell_{2}:E_{2}(f) = \\left(\\sum_{k = 1}^{n}\\lvert f(x_{k}) - y_{k}\\lvert\\right)^{\\frac{1}{2}}$,\n",
    "<br>\n",
    "where $f(x) = mx - b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712923a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing Gradient Descent\n",
    "Let $\\ell_{i}(m,b)$, where $i = 1,2$, be the function gradient descent is being applied to.\n",
    "<br>\n",
    "Start with an initial guess and a defined constant learning rate ($\\delta$):\n",
    "$$\n",
    "m_{0} = \\dots,\\;b_{0} = \\dots,\\;\\delta = \\dots\n",
    "$$\n",
    "At each iteration:\n",
    "$$\n",
    "m_{k + 1} = m_{k} - \\delta\\cdot\\frac{\\partial \\ell_{i}}{\\partial m} \\\\\n",
    "b_{k + 1} = b_{k} - \\delta\\cdot\\frac{\\partial \\ell_{i}}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f9976",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Road Map\n",
    "1. Create **two** linear regression functions that:\n",
    "<br>\n",
    "(a) Uses the specified loss function for gradient descent.\n",
    "<br>\n",
    "(b) Takes in inputs: ($x$, $y$, $m_0$, $b_0$, $\\delta$, epochs).\n",
    "<br>\n",
    "(b) Produces outputs: ($m$, $b$, error).\n",
    "2. Load in data into our main program, isolate desired data, and **split data into training/testing sets**.\n",
    "3. Train algorithm(s) with specified arguements and plot desired results.\n",
    "<br>\n",
    "<font color='red'>NOTE</font>: epochs -> # of iterations training data is passed through an algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8265c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "def algorithm_i(x, y, m, b, L, epochs = 1000):\n",
    "    # Initialize error array\n",
    "    for i in range(epochs):\n",
    "        # Define model: y_pred = mx - b\n",
    "        # Calculate the partial derivative of the loss function w.r.t m (dldm)\n",
    "        # Calculate the partial derivative of the loss function w.r.t b (dldb)\n",
    "        m = m - L * dldm # Update m\n",
    "        b = b - L * dldb # Update b\n",
    "        err[i] = sum(abs(y_pred - y)) !!!or!!! sum(abs(y_pred - y))**(1/2) # Calculate error\n",
    "    return m, b, err # Return calculated coefficients and error (array)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c26957b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caveats\n",
    "1. Too small of a learning rate -> takes too long to reach a minimum.\n",
    "2. Too large of a learning rate -> drastic updates/divergent behavior\n",
    "3. Gradient descent does **not** guarentee that the minimum you are reaching is the **global** minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c874da4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Test your algorithm on the dataset almost_linear.csv (fit one set of data either TV vs sales or Radio vs sales) using at most three different learning rates. Plot $\\ell_{1}$ and $\\ell_{2}$ errors as a function of number of iterations of your algorithm. Plot learning rate as a function of the number of iterations in your algorithm. State values for $m$, $b$, and total error for each objective/loss function and learning rate combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60645d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "# Import packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "...\n",
    "# Load in data\n",
    "# Isolate data\n",
    "X = df.loc[:,'sales'].values.reshape(-1, 1)\n",
    "Y = df.loc[:,'up to you'].values.reshape(-1, 1)\n",
    "# Split the data into training/testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "# Initial guesses for coefficients\n",
    "m0, b0 = ..., ...\n",
    "# Learning step sizes\n",
    "L = [...]\n",
    "# Plot data (mean absolute error)\n",
    "plt.figure()\n",
    "for i in range(len(L)):\n",
    "    # Train Gradient Descent\n",
    "    m_1, b_1, MAE = algorithm_1(X_train, Y_train, m0, b0, L[i])\n",
    "    plt.plot(MAE, label = f'L = {L[i]}')\n",
    "    # Print data\n",
    "    print(f'm = {m_1[0]}, b = {b_1[0]}, L = {L[i]}, Error = {MAE[-1]}')\n",
    "# Plot data (least squared error)\n",
    "plt.figure()\n",
    "for i in range(len(L)):\n",
    "    # Train Gradient Descent\n",
    "    m_2, b_2, LSE = algorithm_2(X_train, Y_train, m0, b0, L[i])\n",
    "    plt.plot(LSE, label = f'L = {L[i]}')\n",
    "    # Print data\n",
    "    print(f'm = {m_2[0]}, b = {b_2[0]}, L = {L[i]}, Error = {LSE[-1]}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f0c3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimal learning rate\n",
    "For most applications, there exists an optimal learning rate for gradient descent at each iteration step; however, only a subset of those applications allow for a computationally inexpensive search for that rate. So, assuming we could (and want to):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17b041",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $f(\\vec{x})$ be the function gradient descent is being applied to.\n",
    "<br>\n",
    "At each iteration:\n",
    "$$\n",
    "\\vec{x}_{k + 1}(\\delta) = \\vec{x}_{k} - \\delta\\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "In using this expression, we have:\n",
    "$$\n",
    "F(\\delta) = f(\\vec{x}_{k + 1}(\\delta))\n",
    "$$\n",
    "Thus, the optimal learning rate can be found by setting $\\frac{dF}{d\\delta} = 0$ and solving for $\\delta$. Hence,\n",
    "$$\n",
    "\\frac{dF}{d\\delta} = -\\nabla f(\\vec{x}_{k + 1})\\nabla f(\\vec{x}_{k}) = 0 \\implies \\delta = \\dots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed8243",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "def algorithm_i(x, y, m, b, L, epochs = 1000, opt_L = False):\n",
    "    if (opt_L == True):\n",
    "        # Initialize error and learning rate array\n",
    "        for i in range(epochs):\n",
    "            # Find the optimal learning rate\n",
    "            # Repeat previously illustrated code\n",
    "    else:\n",
    "        # Previously illustrated code\n",
    "    return m, b, err, L # Return ... and learning rate (array or float)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76176326",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. Compare your results with a standard linear regression solver like sklearn.linear_model.LinearRegression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07dfb59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "# Import packages\n",
    "from sklearn import linear_model\n",
    "...\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, Y_train)\n",
    "# Make prediction using the testing set\n",
    "Y_pred = regr.predict(X_test)\n",
    "for i in range(len(L)):\n",
    "    # Train Gradient Descent\n",
    "    m_1, b_1, _ = algorithm_1(X_train, Y_train, m0, b0, L[i])\n",
    "    m_2, b_2, _ = algorithm_2(X_train, Y_train, m0, b0, L[i])\n",
    "    # Plot data\n",
    "    plt.figure()\n",
    "    sns.scatterplot()\n",
    "    plt.plot(X_test, Y_pred, label = 'Scikitlearn')\n",
    "    plt.plot(X_test, m_1[0]*X_test - b_1[0], label = 'Mean Absolute Error')\n",
    "    plt.plot(X_test, m_2[0]*X_test - b_2[0], label = 'Least Squared Error')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c03099",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Selection\n",
    "1. Explore the data (https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). Plot the distribution of median house value. Plot median house value vs total bedrooms. Plot the correlation matrix. Discuss plot results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc4b51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "# Load in data\n",
    "data = pd.read_csv('cal_housing.data', header = None)\n",
    "# Set column names\n",
    "data.columns = [...]\n",
    "sns.histplot() # Plot data 1\n",
    "sns.scatterplot() # Plot data 2\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "corr_matrix = data.corr() # Find correlation matrix\n",
    "sns.heatmap() # Plot data 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b7179",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Explore three different linear regression models: these can include least squares, lasso, ridge, elastic net, or any other linear (or nonlinear) regression model. Perform a grid search on the regularization parameters, and use five fold cross validation to find parameter values that minimize the error on your test data for each of the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0be55a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### k-Fold Cross Validation\n",
    "**Problem**: How can we compare different models, or the same model with different parameter(s), in a fair and robust way?\n",
    "<br>\n",
    "<br>\n",
    "**Idea**: Partition a dataset into a training set and a test set $k$ different ways and average the \"scores\" of each of the models (or parameters) and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86db664",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid Search\n",
    "**Problem**: What is the best parameter, or parameters, for a specific model?\n",
    "<br>\n",
    "<br>\n",
    "**Idea**: Build a model with **every** possible parameter value (defined by the user), and compare which parameter value performed the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bebfce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "# Import packages\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Isolate data\n",
    "# Define the parameter we wish to optimize\n",
    "parameters = {'alpha':np.linspace(start, stop, number of steps)}\n",
    "Lasso = linear_model.Lasso() # Define the model we wish to test\n",
    "# Define a grid search that will use 5-fold cross validation\n",
    "Lasso_reg = GridSearchCV(Lasso, parameters,\n",
    "                         scoring = 'neg_root_mean_squared_error',\n",
    "                         cv = 5,\n",
    "                         return_train_score = True)\n",
    "Lasso_reg.fit(X,Y) # Fit the grid search\n",
    "print(Lasso_reg.best_estimator_) # Print best alpha parameter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42f4e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. (cont.) For each model, plot a) regularization parameter value vs. error on test data and error on training data b) regularization parameter value vs predictor coefficient values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde5512",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code (a)\n",
    "```python\n",
    "# Finding regularization parameter value vs. error on test data\n",
    "Lasso_meanTest = abs(Lasso_reg.cv_results_['mean_test_score'])\n",
    "# Plotting data (1)\n",
    "plt.figure()\n",
    "plt.plot(parameters['alpha'], Lasso_meanTest, label = 'LASSO')\n",
    "# Finding regularization parameter value vs. error on training data\n",
    "Lasso_meanTrain = abs(Lasso_reg.cv_results_['mean_train_score'])\n",
    "# Plotting data (2)\n",
    "plt.figure()\n",
    "plt.plot(parameters['alpha'], Lasso_meanTrain, label = 'LASSO')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de31f6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code (b)\n",
    "```python\n",
    "# Split the data into training/testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "# Defining empty lists for coefficients\n",
    "Lasso_coef = []\n",
    "# Finding regularization paramter value vs. predictor coefficient value\n",
    "for a in parameters['alpha']:\n",
    "    # LASSO\n",
    "    Lasso = linear_model.Lasso(alpha = a)\n",
    "    Lasso.fit(X_train, Y_train)\n",
    "    Lasso_Y_pred = Lasso.predict(X_test)\n",
    "    Lasso_coef.append(Lasso.coef_)\n",
    "plt.plot(parameters['alpha'], Lasso_coef, label = 'LASSO')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830605c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. Plot test errors for all three models for the optimal parameters chosen in question 2. Choose the model that best fits the data. Discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eccd1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution Pseudo Code\n",
    "```python\n",
    "plt.bar(['LASSO', ...], [abs(Lasso_reg.best_score_), ...])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
